{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clickstream Clustering using Weighted Longest Common Subsequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is scientific article implementation: Arindam Banerjee and Joydeep Ghosh, _Clickstream Clustering Using Weighted Longest Common Subsequences_, (2001) [link here](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.135.9092).\n",
    "\n",
    "The implementation is divided into 3 parts:\n",
    "* First, paths similarity measure is calculated for all paths\n",
    "* Second, similarity graph is constructed\n",
    "* Third, graph partitioning algorithm is executed\n",
    "\n",
    "As it was impossible to get the same as in article data from [www.sulekha.com](https://www.sulekha.com), the weblogs from [www.bodyworlds.nl](https://www.bodyworlds.nl/nl) were considered instead. This choice is temporal, and data from e.g. NASA Kennedy Space Center [www.ita.ee.lbl.gov](http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html) should be used instead as those are the classical choice for web mining applications. The [www.bodyworlds.nl](https://www.bodyworlds.nl/nl) was chosen as these weblogs require minimum initial preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statistics import mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data format - list of tuples\n",
    "# [('path1', time1), ('path2', time2), ('path3', time3), ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    {'session_id': 1, \n",
    "     'page_views': [('website', 20), ('tickets', 10), ('convious', 2), ('choose-your-tickets', 10), ('cart', 30), ('order', 1), ('get-your-tickets', 3)]},\n",
    "    {'session_id': 2, \n",
    "     'page_views': [('website', 5), ('about', 30)]},\n",
    "    {'session_id': 3, \n",
    "     'page_views': [('website', 45), ('tickets', 45), ('convious', 34), ('choose-your-tickets', 55)]},\n",
    "    {'session_id': 4, \n",
    "     'page_views': [('website', 23), ('about', 45), ('tickets', 23), ('contact', 21), ('convious', 4)]}\n",
    "]\n",
    "test_df = pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_views</th>\n",
       "      <th>session_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(website, 20), (tickets, 10), (convious, 2), ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(website, 5), (about, 30)]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(website, 45), (tickets, 45), (convious, 34),...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(website, 23), (about, 45), (tickets, 23), (c...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          page_views  session_id\n",
       "0  [(website, 20), (tickets, 10), (convious, 2), ...           1\n",
       "1                        [(website, 5), (about, 30)]           2\n",
       "2  [(website, 45), (tickets, 45), (convious, 34),...           3\n",
       "3  [(website, 23), (about, 45), (tickets, 23), (c...           4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://github.com/man1/Python-LCS\n",
    "# get the matrix of LCS lengths at each sub-step of the recursive process\n",
    "# (m+1 by n+1, where m=len(list1) & n=len(list2) ... it's one larger in each direction \n",
    "# so we don't have to special-case the x-1 cases at the first elements of the iteration\n",
    "def lcs_mat(list1, list2):\n",
    "    m = len(list1)\n",
    "    n = len(list2)\n",
    "    # construct the matrix, of all zeroes\n",
    "    mat = [[0] * (n+1) for row in range(m+1)]\n",
    "    # populate the matrix, iteratively\n",
    "    for row in range(1, m+1):\n",
    "        for col in range(1, n+1):\n",
    "            if list1[row - 1] == list2[col - 1]:\n",
    "                # if it's the same element, it's one longer than the LCS of the truncated lists\n",
    "                mat[row][col] = mat[row - 1][col - 1] + 1\n",
    "            else:\n",
    "                # they're not the same, so it's the the maximum of the lengths of the LCSs of the two options (different list truncated in each case)\n",
    "                mat[row][col] = max(mat[row][col - 1], mat[row - 1][col])\n",
    "    # the matrix is complete\n",
    "    return mat\n",
    "\n",
    "# backtracks all the LCSs through a provided matrix\n",
    "def all_lcs(lcs_dict, mat, list1, list2, index1, index2):\n",
    "    #calculate recursively\n",
    "    if (index1 == 0) or (index2 == 0): # base case\n",
    "        return [[]]\n",
    "    elif list1[index1 - 1] == list2[index2 - 1]:\n",
    "        # elements are equal! Add it to all LCSs that pass through these indices\n",
    "        lcs_dict[(index1, index2)] = [prevs + [list1[index1 - 1]] for prevs in all_lcs(lcs_dict, mat, list1, list2, index1 - 1, index2 - 1)]\n",
    "        return lcs_dict[(index1, index2)]\n",
    "    else:\n",
    "        lcs_list = [] # set of sets of LCSs from here\n",
    "        # not the same, so follow longer path recursively\n",
    "        if mat[index1][index2 - 1] >= mat[index1 - 1][index2]:\n",
    "            before = all_lcs(lcs_dict, mat, list1, list2, index1, index2 - 1)\n",
    "            for series in before: # iterate through all those before\n",
    "                if not series in lcs_list: lcs_list.append(series) # and if it's not already been found, append to lcs_list\n",
    "        if mat[index1 - 1][index2] >= mat[index1][index2 - 1]:\n",
    "            before = all_lcs(lcs_dict, mat, list1, list2, index1 - 1, index2)\n",
    "            for series in before:\n",
    "                if not series in lcs_list: lcs_list.append(series)\n",
    "        lcs_dict[(index1, index2)] = lcs_list\n",
    "        return lcs_list\n",
    "\n",
    "# return a set of the sets of longest common subsequences in list1 and list2\n",
    "def lcs(list1, list2):\n",
    "    # mapping of indices to list of LCSs, so we can cut down recursive calls enormously\n",
    "    mapping = dict()\n",
    "    return all_lcs(mapping, lcs_mat(list1, list2), list1, list2, len(list1), len(list2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return filtered subsequences with time\n",
    "def time_lcs(subsequence, list_of_dict):\n",
    "    # TODO: decide what to do if several subsequences of the same length are found\n",
    "    subsequence = subsequence[0]\n",
    "    return [t for t in list_of_dict if t[0] in subsequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute composed similarity measure\n",
    "def similarity(list_of_paths1, list_of_paths2):\n",
    "    paths1 = [seq[0] for seq in list_of_paths1]\n",
    "    T1 = sum([seq[1] for seq in list_of_paths1])\n",
    "    \n",
    "    paths2 = [seq[0] for seq in list_of_paths2]\n",
    "    T2 = sum([seq[1] for seq in list_of_paths2])\n",
    "    \n",
    "    common_subsequence = lcs(paths1, paths2)\n",
    "    path_subsequence1 = time_lcs(common_subsequence, list_of_paths1)\n",
    "    t1 = sum([seq[1] for seq in path_subsequence1])\n",
    "    path_subsequence2 = time_lcs(common_subsequence, list_of_paths2)\n",
    "    t2 = sum([seq[1] for seq in path_subsequence2])\n",
    "    \n",
    "    return s_similarity(path_subsequence1, path_subsequence2) * s_importance(t1, T1, t2, T2)\n",
    "\n",
    "# compute similarity component\n",
    "def s_similarity(t_alpha, t_beta):\n",
    "    s = [min(t_a[1], t_b[1])/max(t_a[1], t_b[1]) for t_a, t_b in zip(t_alpha, t_beta)]\n",
    "    return mean(s)\n",
    "\n",
    "# compute importance component\n",
    "def s_importance(t_alpha, T_alpha, t_beta, T_beta):\n",
    "    return math.sqrt(t1/T1 * t2/T2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_graph(df, edge_threshold=.1):\n",
    "    # create graph object\n",
    "    g = nx.Graph()\n",
    "    \n",
    "    # add nodes\n",
    "    g.add_nodes_from(list(df.session_id))\n",
    "    \n",
    "    # add egdes\n",
    "    for session_1 in range(len(df)):\n",
    "        for session_2 in range(session_1 + 1, len(df)):\n",
    "            edge_weight = similarity(df.iloc[session_1].page_views, df.iloc[session_2].page_views)\n",
    "            if edge_weight >= edge_threshold:\n",
    "                g.add_edge(df.iloc[session_1].session_id, df.iloc[session_2].session_id, weight=edge_weight)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = similarity_graph(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metis with min-cut and balancing constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
